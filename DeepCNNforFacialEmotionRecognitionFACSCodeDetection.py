# -*- coding: utf-8 -*-
"""s3880522.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XaNA4Dm_Y8o823rG8CPW94ktTIw96xKX

---
# <div align="center"><font color='blue'>  </font></div>
# <div align="center"><font color='blue'> COSC 2779 | Deep Learning Assignment 1 </font></div>
## <div align="center"> <font color='blue'> Submitted by : **Jyoti, s3880522**</font></div>
## <div align="center"> <font color='blue'> Topic : Deep CNN for Facial Emotion Recognition &  FACS Code Detection</font></div>
---

# Introduction - Facial Emotion Recognition &  FACS Code Detection

In this Jupyter notebook, I'll delve into the exciting domain of facial expression analysis using deep learning. My objective is to create a robust Convolutional Neural Network (CNN) capable of simultaneously predicting high-level emotions and detecting Facial Action Coding System (FACS) codes within facial images.

The dataset is a curated subset of the renowned CK+ dataset. Comprising 560 labeled images from 123 subjects, this dataset serves as the foundation for our model training and evaluation.

My notebook is structured as follows:

1. **Data Preprocessing:** I'll start by loading and preparing the dataset, including resizing and normalizing images for optimal model input. It includes data loading as well.

2. **Model Architecture:** Defining the architecture of CNN is the next step, allowing us to harness its potential for emotion recognition and FACS code detection.


3. **Training and Evaluation:** This section will showcase the model training process using the preprocessed dataset. We'll evaluate its performance on both emotion classification and FACS code detection tasks.

4. **Results and Discussion:** Upon completion of training, we'll analyze and interpret the model's performance, discussing insights and potential areas of improvement.

My journey through this notebook promises insights into the intricate world of facial expressions, deep learning and my best understanding about the topic given.

#Problem Description &  Objecive
>  Explore a real dataset to practice the typical deep learning
process - CK dataset, and  develop an end-to-end trained deep convolutional neural network (CNN) to identify facial emotions and FACS (Facial Action Coding
System) codes present in an image. I  will use a subset of the The Extended
Cohn-Kanade Dataset (CK+) provided on canvas as part of assignment.
In a nutshell :  
- Develop a deep learning system to solve a real-world problem - Facial emption detection
- Analyse  output of the algorithm(s)
- Research how to extend the DL techniques and fine tune the model
- Provide an ultimate judgement of the final trained model that you would use in a real-world setting.


###Objective

> The task is to design a thorough experiment and determine how well the following two attributes can be predicted:
- High Level Emotion: Does the image show “Positive”, “Negative” or “surprised”
subject?
- FACS codes: Does the image show a particular facial action (FACS code)? This
is a yes(1)/no(0) output.

# Setting up the Notebook

I will' first load the packages I need to build the baseline model as part of this task. Here I set up the environment for working with TensorFlow and related libraries, and imported various modules necessary for data manipulation, visualization, and machine learning tasks.

- Importing TensorFlow and setting up optimization constants.
- Importing NumPy and Pandas for numerical computations and data manipulation.
- Importing TensorFlow Datasets for accessing pre-built datasets.
- Importing modules for handling file paths and directories.
- Importing modules for visualization using Matplotlib.
- Importing functions for controlling display in IPython environments.
- Importing Scikit-learn's function for splitting datasets into training and testing sets.
"""

import tensorflow as tf
AUTOTUNE = tf.data.experimental.AUTOTUNE
import numpy as np
import pandas as pd

import tensorflow_datasets as tfds
import pathlib
import shutil
import tempfile

from  IPython import display
from matplotlib import pyplot as plt

import matplotlib.ticker as ticker
from sklearn.model_selection import train_test_split

"""#Setting up Instrumentation

I will use the tensor board to view the learning curves, activation and weight hostograms.
- The code below sets up a temporary directory for storing TensorBoard logs,
- Removes any previous logs from that directory, loads the TensorBoard extension for IPython,
- And then opens an embedded TensorBoard viewer within the IPython environment to visualize the training progress and insights related to the TensorFlow models stored in the specified logdir.

This part  especially useful for monitoring and analyzing various aspects of my choosen model's performance during training, such as loss, accuracy, and more.

I am creating my own function to plot the models training history , it will be called after  training the model in order to plot graphs.

- I have defined a function named plotter that is used for plotting training and validation metrics of different models' training history.
- The function takes in a dictionary containing training histories (typically the output of model.fit()),
- the desired metric to plot (defaulting to 'binary_crossentropy'),
- and the desired y-axis limits (defaulting to [0.0, 1.0]).

Overall , the plotter function is a utility function to visualize the training progress of multiple models by plotting their training and validation metrics over epochs, and it is particularly useful for comparing and analyzing the performance of different models during training.
"""

from itertools import cycle
def plotter(history_hold, metric = 'binary_crossentropy', ylim=[0.0, 1.0]):
  cycol = cycle('bgrcmk')
  for name, item in history_hold.items():
    y_train = item.history[metric]
    y_val = item.history['val_' + metric]
    x_train = np.arange(0,len(y_val))

    c=next(cycol)

    plt.plot(x_train, y_train, c+'-', label=name+'_train')
    plt.plot(x_train, y_val, c+'--', label=name+'_val')

  plt.legend()
  plt.xlim([1, max(plt.xlim())])
  plt.ylim(ylim)
  plt.xlabel('Epoch')
  plt.ylabel(metric)
  plt.grid(True)

"""#Dataset Loading &  Exploration of  Extended Cohn-Kanade (CK+) Dataset

The Extended Cohn-Kanade (CK+) Dataset is a comprehensive collection of facial behavior recordings aimed at studying human emotion expressions and the Facial Action Coding System (FACS).


1.   The dataset captured the facial behaviors of 210 adults, spanning an age range of 18 to 50 years.
2.  The demographic composition of the participants includes: 69% female participants.
A mix of 81% Euro-American, 13% Afro-American, and 6% other groups.
3. Participants were instructed by an experimenter to perform 23 distinct facial displays. These displays encompassed both single action units and combinations of action units. Here, each display sequence initiated and concluded with a neutral facial expression. Any exceptions to this pattern were duly noted.
4. The dataset features image sequences captured from both frontal and 30-degree views.

5. The images were digitized into pixel arrays of varying dimensions: Pixel arrays measured 640x490 or 640x480 pixels.

The pixel arrays were represented in two formats:

- 8-bit grayscale, capturing nuanced intensity variations.
- 24-bit color, preserving color information.

The dataset's meticulous recording of diverse facial expressions provides an invaluable resource for :
- Training deep learning models to recognize emotions.
Detecting specific facial actions using FACS codes.
-  To gain insights into the complex domain of facial expression analysis and emotion recognition


The data set information is referred from paper :
[Paper :  P. Lucey, J. F. Cohn, T. Kanade, J. Saragih, Z. Ambadar and I. Matthews, "The Extended Cohn-Kanade Dataset (CK+): A complete dataset for action unit and emotion-specified expression," 2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition - Workshops, San Francisco, CA, USA, 2010, pp. 94-101, doi: 10.1109/CVPRW.2010.5543262.](https://)



For the given Extended Cohn-Kanade (CK+) Dataset, I am considering a combination of metrics and performance evaluation techniques to assess the effectiveness of  facial expression recognition model. Here are some key metrics and performance measures to consider:

- Accuracy: Accuracy is a common metric used to evaluate classification models, including those for facial expression recognition. It measures the proportion of correctly classified samples out of the total number of samples.

- Precision, Recall, and F1-Score: These metrics are particularly useful when dealing with imbalanced datasets or when certain classes are more important than others. Precision measures the proportion of correctly predicted positive instances out of all instances predicted as positive. Recall (also known as sensitivity) measures the proportion of correctly predicted positive instances out of all actual positive instances. F1-score is the harmonic mean of precision and recall.

- Confusion Matrix: The confusion matrix provides a detailed breakdown of the model's predictions. It shows the number of true positive, true negative, false positive, and false negative predictions for each class. This is valuable for understanding where your model excels and where it struggles.

I will choose a final metric and loss as per my data set and model during model evolution.

### Loading the dataset

As a next step,   I am going to load dataset. I will mount   google drive and access the data which is stored in my perosnal google drive.
"""

from google.colab import drive
drive.mount('/content/drive')

!cp /content/drive/'My Drive'/DeepLearningAssignment/data_labels.csv .

!cp /content/drive/'My Drive'/DeepLearningAssignment/cohn-kanade-images.zip .

!unzip -q -o cohn-kanade-images.zip

!rm cohn-kanade-images.zip

# Define the directory path for the CSV file
directory_path= '/content/data_labels.csv'
# Read the CSV file into a DataFrame
data = pd.read_csv(directory_path)
data.head()

# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard
import tensorflow as tf
import datetime

log_dir = '/content/drive/My Drive/DeepLearningAssignment/ResNetLogDirectory'

# %load_ext tensorboard
import tensorflow as tf
import datetime

# Create a TensorBoard callback
tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)

# Start TensorBoard

# %tensorboard --logdir Your_Logs_Directory

data.describe()

data.info()

"""###Data Observation

Here's a brief overview of the columns:

subject: A categorical column with non-null values representing subjects or individuals.
sequence: An integer column representing a sequence identifier.
image_index: An integer column representing an image index.
filepath: A string column containing file paths.
file_prefix: A string column containing file prefixes.
Columns AU17 to AU26: Integer columns containing values related to different AU (Action Unit) codes.
high_level_emotion: A categorical column with non-null values representing high-level emotions.

> I dealing with the CK dataset and  chosen different loss functions for different outputs of your model ('emotion_output' and 'facs_output'). Let's break down your choice:

- emotion_output: I  will be using 'categorical_crossentropy' as the loss function for this output. My model's goal is to predict categorical emotions (such as happy, sad, surprise.). The reaosn is Categorical cross-entropy is a common choice for multi-class classification problems like emotion recognition, where each input sample belongs to exactly one class.

- facs_output: I will be using 'binary_crossentropy' as the loss function for this output. Because my model is dealing with binary classification for the Facial Action Coding System (FACS) labels. The FACS system categorizes facial expressions into various action units, each representing a specific facial muscle movement. Since each action unit can be either active (1) or inactive (0), binary cross-entropy is suitable for this type of binary classification problem.

### Dropping the columns

As I can see the CK+ (Cohn-Kanade) database, certain columns can be considered for removal as per significance to reudce model complexity.

> - 'sequence' Column: Rationale: The 'sequence' column represents recording sessions for each subject. My analysis focuses solely on the individual facial expressions and emotions, the specific recording sessions might not be relevant.

> - 'image_index' Column: Rationale: Similar to the 'sequence' column, the 'image_index' column is useful primarily for chronological order within a sequence. My analysis doesn't require considering the temporal aspect of the images, this column might not add substantial value to your objectives.

> - 'file_prefix' Column: Rationale: The 'file_prefix' column serves as a prefix for filenames. While it aids in constructing file paths, it don't not directly contribute to emotion recognition or facial action detection tasks. This column could be omitted if it doesn't align with your analysis goals.

> - Demographic Columns (e.g., 'subject'): Rationale: I am primarily interested in emotion recognition or facial action detection irrespective of subject demographics, subject columns could be removed to simplify the dataset. Dropping columns will help declutter the dataset and improve processing efficiency.
"""

columns_to_drop = ['subject', 'sequence', 'image_index', 'file_prefix']
data = data.drop(columns=columns_to_drop)

data.head()

import pandas as pd

# Assuming 'data' is your DataFrame
# Check if 'high_level_emotion' column has any null values
has_null_values = data['high_level_emotion'].isnull().any()

if has_null_values:
    print("'high_level_emotion' column has null values.")
else:
    print("'high_level_emotion' column does not have null values.")

"""##Creating label for high level emotion column
> - The reason for creating this numerical representation is to convert the categorical emotion labels into a format that my choosen machine learning algorithms can work with labels. Many algorithms require numerical inputs, and using numerical values for categorical labels will help in training predictive models.
- In this case, using numerical values like -1, 0, and 1 represents the emotions on a scale and enables the application of various classification techniques for tasks like emotion recognition
"""

# Define a dictionary to map emotion names to numerical values
emotion_to_num = {'negative': -1, 'surprise': 0, 'positive': 1}

# Apply the emotion mapping to create the new column
data['emotion_num'] = data['high_level_emotion'].map(emotion_to_num)

# Display the DataFrame to verify the new column
print(data.head(5))

# Print the list of columns
print(data.columns)

"""## Image Visualization and Data Description

I have defined a code that iterates through the first 10 rows of the DataFrame, opens and displays images from their file paths using PIL and Matplotlib, arranges them in a 2x5 grid of subplots, and then shows the grid of images in a single figure. This is useful for visually inspecting a subset of the images stored in the DataFrame.
"""

from PIL import Image
import matplotlib.pyplot as plt

# Iterate through the first 10 rows
for index in range(10):
    # Get the filepath from the DataFrame
    image_filepath = data.iloc[index]['filepath']

    # Open and display the image
    image = Image.open(image_filepath)

    plt.subplot(2, 5, index + 1)  # Create a 2x5 grid of subplots
    plt.imshow(image)
    plt.axis('off')

plt.tight_layout()
plt.show()

from PIL import Image

# Choose the index of the image you want to describe
chosen_index = 0  # Change this to the desired index

# Load the information from the CSV file using the chosen index
image_filepath = data.iloc[chosen_index]['filepath']
emotion = data.iloc[chosen_index]['high_level_emotion']
emotion_num = data.iloc[chosen_index]['emotion_num']

# Open the image using PIL
image = Image.open(image_filepath)

# Print the image information and dimensions
print("Image Information:")
print(f"   Filepath: {image_filepath}")
print(f"   Emotion: {emotion}")
print(f"   Emotion Number: {emotion_num}")
print(f"   Image Size: {image.size}")
print(f"   Image Mode: {image.mode}")
print(f"   Image Format: {image.format}")

# Create a set to store unique image sizes
unique_image_sizes = set()

# Iterate through the DataFrame to collect unique image sizes
for index in range(len(data)):
    image_filepath = data.iloc[index]['filepath']
    image = Image.open(image_filepath)
    image_size = image.size
    unique_image_sizes.add(image_size)

# Print the unique image sizes
print("\nUnique Image Sizes:")
for size in unique_image_sizes:
    print(size)

"""###Observation - Image Information:

- Filepath: The path to the chosen image file on the system.
Emotion: The emotional label associated with the image. In this case, it's "negative."
- Emotion Number: The numerical representation of the emotion. For "negative," the emotion number is -1.
- Image Size: The dimensions of the image in pixels. The chosen image has dimensions of 640 pixels in width and 490 pixels in height.
- Image Mode: The color mode of the image. 'L' indicates grayscale.
- Image Format: The file format of the image. The chosen image is in PNG format.

Unique Image Sizes:

The dataset contains images with various sizes:
(720, 480): A common size with dimensions of 720x480 pixels.
(640, 490): Similar to the chosen image, with dimensions of 640x490 pixels.
(640, 480): Another common size with dimensions of 640x480 pixels.
"""

# Display the number of rows and columns in the dataset
print("Number of rows:", data.shape[0])
print("Number of columns:", data.shape[1])

# Display the column names in the dataset
print("\nColumn Names:")
print(data.columns)

# Display the first few rows of the dataset
print("\nFirst Few Rows:")
print(data.head())

# Display summary statistics of the dataset
print("\nSummary Statistics:")
print(data.describe())

print("\n******************************")

# Display unique values in the 'high_level_emotion' column
print("\nUnique Emotions:")
print(data['high_level_emotion'].unique())

# Display counts of each unique emotion
print("\nEmotion Counts:")
print(data['high_level_emotion'].value_counts())

# Display AU units with value 1 and their respective emotion for the first few rows
print("\n******************************")
print("\nAU Units with Value 1:")
for index in range(5):  # Display for the first 5 rows
    au_columns = [col for col in data.columns if col.startswith('AU') and data.loc[index, col] == 1]
    emotion = data.loc[index, 'high_level_emotion']
    au_units = ', '.join(au_columns)
    print(f"Row {index + 1} Emotion: {emotion} , AU Units: {au_units}")

print("\n******************************")

"""##Action Units Analysis

- Unique Emotions:The dataset contains three unique emotions: 'negative,' 'surprise,' and 'positive.'The array ['negative', 'surprise', 'positive'] represents these unique emotions.

- Emotion Counts:The number of occurrences of each emotion in the dataset:
328 instances of 'negative' emotion.
121 instances of 'positive' emotion.
111 instances of 'surprise' emotion.
AU Units with Value 1:

- The dataset includes rows with specific emotion labels and the associated Action Units (AU) that have a value of 1.For example:
Row 1: Emotion is 'negative,' and AU Unit is 'AU17.'
Row 2: Emotion is 'negative,' and AU Units are 'AU1,' 'AU2,' and 'AU25.'
Row 3: Emotion is 'surprise,' and AU Units are 'AU1,' 'AU2,' 'AU25,' and 'AU27.'
Row 4: Emotion is 'negative,' and AU Units are 'AU17' and 'AU4.'
Row 5: Emotion is 'negative,' and AU Units are 'AU17,' 'AU4,' 'AU7,' 'AU23,' and 'AU24.'

##Observation - Emotion Counts & Biasness
The "Emotion Counts" section here shows the number of occurrences of each unique emotion label in your dataset. It's a count of how many times each emotion appears. Looking at the counts:

- 'negative': 328 occurrences
- 'positive': 121 occurrences
- 'surprise': 111 occurrences

> Biasness Explanation: The term "biasness" in this context refers to an imbalance or uneven distribution of the different emotion labels in your dataset. In this case, the dataset is biased towards the 'negative' emotion label, as it appears more frequently than the other labels. This can potentially impact the performance of my machine learning model trained on this dataset, as the model might become better at recognizing the 'negative' emotion but struggle with the other emotions due to fewer examples.

> When working with biased datasets further, it's important for me to be aware of the potential bias and its implications. Addressing biasness could involve techniques like data augmentation, resampling, or using specialized loss functions to give more weight to underrepresented classes. This helps to ensure that the model generalizes well to all emotions rather than being overly skewed towards the majority class ('negative' in this case).
"""

# Extract the 'high_level_emotion' column
emotion_labels = data['high_level_emotion']

# Plot a histogram of emotion labels
plt.figure(figsize=(8, 6))
plt.hist(emotion_labels, bins=20, color='blue', alpha=0.7)
plt.xlabel('Emotion Labels')
plt.ylabel('Frequency')
plt.title('Histogram of Emotion Labels')
plt.show()

"""## Graphical visualization - Histogram
I read the emotion labels, and then created and displayed a histogram to visualize the distribution of those emotion labels. The histogram helped to  understand the frequency of different emotions

It is very cleary visible in grpah that in the given data, I have more data related to one item - negative emotions.
"""

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt


# Select the AU columns
au_columns = ['AU17', 'AU1', 'AU2', 'AU25', 'AU27', 'AU4', 'AU7', 'AU23', 'AU24', 'AU6', 'AU12', 'AU15', 'AU14', 'AU11', 'AU26']

# Calculate the correlation matrix
correlation_matrix = data[au_columns].corr()

# Create a heatmap
plt.figure(figsize=(12, 10))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)
plt.title('Correlation Heatmap of AU Columns')
plt.show()

"""##Visuaization Heatmap - Correlation Range:

The correlation coefficient ranges from -1 to 1.
- Positive values (closer to 1) indicate a positive correlation: when one AU value increases, the other tends to increase as well.
- Negative values (closer to -1) indicate a negative correlation: when one AU value increases, the other tends to decrease.
- A correlation coefficient of 0 indicates no linear correlation between the two AU columns.


> Strong Positive Correlations:
> - There is a strong positive correlation between AU 1 and AU 2.
When AU 1 is activated (indicating brow raiser), AU 2 (inner brow raiser) is also likely to be activated. These AUs often work together to create a combined expression.
AU 1 and AU 27:

> - AU 1 and AU 27 show a strong positive correlation.
Activation of AU 1 (brow raiser) is often associated with the activation of AU 27 (brow furrower). This could suggest the presence of mixed expressions like surprise with concern.
AU 2 and AU 27:

 > - Similar to the previous observation, AU 2 (inner brow raiser) and AU 27 (brow furrower) exhibit a strong positive correlation.
These AUs might combine to express emotions like puzzlement or intensity.
AU 6 and AU 12:

> - AU 6 (cheek raiser) and AU 12 (lip corner puller) show a strong positive correlation. Activation of AU 6 is often accompanied by the activation of AU 12.
Together, they could indicate a genuine or intense smile.

> - AU 23 and AU 24: AU 23 (lip tightener) and AU 24 (lip presser) have a strong positive correlation. Activation of one tends to coincide with the activation of the other. These AUs could contribute to lip compression expressions.

> - AU 4 and AU 7: There's a strong positive correlation between AU 4 (brow lowerer) and AU 7 (lid tightener).Activation of one is often accompanied by the activation of the other. These AUs might contribute to expressions of concern or frustration.

> Strong Negative Correlations:

> - AU 11 and AU 26:AU 11 (nasolabial deepener) and AU 26 (jaw drop) exhibit a strong negative correlation. When one is activated, the other tends to deactivate. These AUs could work in opposition to express different emotional intensities.
> - AU 14 and AU 15: AU 14 (dimpler) and AU 15 (lip corner depressor) have a strong negative correlation.This might contribute to expressions involving cheek compression and lip corner pull down.
Non-Linear or Weak Correlations:

> Neutral :
- AU 6 and AU 24: The correlation between AU 6 (cheek raiser) and AU 24 (lip presser) is not as strong. Their activations do not consistently follow a linear pattern. These AUs might play roles in varied expressions, influenced by other factors.

##Graphical Visualization - Line chart
"""

import matplotlib.pyplot as plt

# Calculate the average AU values for each emotion
avg_au_values = data.groupby('high_level_emotion')[['AU1', 'AU2', 'AU4', 'AU6', 'AU7', 'AU11', 'AU12', 'AU14', 'AU15', 'AU17', 'AU23', 'AU24', 'AU25', 'AU26', 'AU27']].mean()

# Create a curve chart
plt.figure(figsize=(10, 6))
avg_au_values.T.plot(kind='line')
plt.title('Average AU Values Across Emotions')
plt.xlabel('AU')
plt.ylabel('Average Value')
plt.legend(title='Emotion')
plt.grid(True)
plt.show()

"""#Data Splitting

Below, I  splitted CK dataset into training, validation, and test sets using the train_test_split function. I have then printed out the shapes of the original dataset and the resulting splits.

- Why Validation data : The validation set is a crucial component in the machine learning pipeline when working with the CK dataset for FACS and emotion detection. It aids in hyperparameter tuning, model selection, preventing overfitting, and assessing how well my model generalizes to new data, all of which are essential for building robust and effective models
"""

from sklearn.model_selection import train_test_split

# Split the dataset into training (60%), validation (20%), and test (20%) sets
train_data_df, temp_df = train_test_split(data, test_size=0.4, random_state=42)
val_data_df, test_data_df = train_test_split(temp_df, test_size=0.5, random_state=42)


print("Shape of the original dataset:", data.shape)
print("Shape of the training set:", train_data_df.shape)
print("Shape of the validation set:", val_data_df.shape)
print("Shape of the test set:", test_data_df.shape)

"""The original dataset has 560 samples and 18 features.
- After splitting, the training set contains 336 samples, while both the validation and test sets contain 112 samples each.
- This division allows for training and evaluating machine learning models using separate datasets, aiding in proper model assessment and generalization.

# Data loader-Data generator & Augmentation

I have defined a DataGenerator class is  to efficiently prepare batches of image data with associated labels for training deep learning models.
- It incorporates data preprocessing, augmentation, and normalization steps to enhance the model's ability to learn from the CK dataset, especially for tasks like FACS label and emotion outcome detection.


> Here's a summary of the key aspects of the code:

> - Class Initialization (__init__): The constructor initializes parameters such as batch size, image dimensions, number of classes, data normalization statistics, and more. It also sets up attributes for data handling, augmentation control, and FACS labels.

> - Data Generation (__data_generation):Generates a batch of preprocessed image data and labels based on provided indexes and image labels.
Images are loaded, resized, and optionally augmented with techniques like rotation, shift, padding, and brightness adjustment.
Data is normalized using the provided mean and standard deviation.

> - Batch Retrieval (__getitem__): Retrieves a batch of preprocessed data and corresponding labels for training. Calls __data_generation to prepare the batch data.

> - Batch Visualization (plot_batch_images): Method to visualize batches of images for inspection and verification purposes.
Images are displayed using Matplotlib subplots.

> - End-of-Epoch Handling (on_epoch_end):Shuffles data indices after each epoch to ensure diverse data presentation during training.

> - Data Augmentation (Augment Parameter): If enabled, applies augmentation techniques to images to enhance model generalization. Techniques include random rotation, shift, padding, brightness adjustment, and cropping.

> - Image Reading and Preprocessing (__read_data_instance): Reads image files, converts to grayscale, resizes, applies augmentation (if required), normalizes, and returns preprocessed images.
"""

import tensorflow.keras as keras
import numpy as np
from scipy.ndimage.interpolation import rotate, shift
from PIL import Image

class DataGenerator(keras.utils.Sequence):
    'Generates data for Keras'
    def __init__(self, data_frame, batch_size=8, dim=(490, 490, 3), n_classes=3, data_mean=0, data_std=1, data_prefix='', shuffle=True, Augment=True, facs_labels=None):
        'Initialization'
        self.dim = dim
        self.batch_size = batch_size
        self.n_classes = n_classes
        self.shuffle = shuffle
        self.Augment = Augment
        self.data_frame = data_frame
        self.image_label = data_frame['emotion_num'].values.tolist()
        self.image_ids = np.arange(len(self.image_label)).tolist()
        self.data_prefix = data_prefix
        self.facs_labels = facs_labels  # FACS labels

        # Data normalization parameters
        self.data_mean = data_mean
        self.data_std = data_std

        self.on_epoch_end()

    def __len__(self):
        'Denotes the number of batches per epoch'
        return int(np.floor(len(self.image_ids) / self.batch_size))

    def __getitem__(self, index):
        'Generate one batch of data for the given index'
        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]
        data_ids_temp = [self.image_ids[k] for k in indexes]
        image_label_temp = [self.image_label[k] for k in indexes]

        X, y = self.__data_generation(data_ids_temp, image_label_temp)
        return X, y

    def on_epoch_end(self):
        'Updates indexes after each epoch'
        self.indexes = np.arange(len(self.image_ids))
        if self.shuffle:
            np.random.shuffle(self.indexes)

    def __data_generation(self, data_ids_temp, image_label_temp):
        'Generates data containing batch_size samples'
        X = np.empty((self.batch_size, *self.dim))
        y = np.empty((self.batch_size), dtype=int)
        y2 = np.empty((self.batch_size, self.facs_labels.shape[1]), dtype=int)  # FACS labels

        for i, ids in enumerate(data_ids_temp):
            X[i,] = self.__read_data_instance(data_ids_temp[i])
            y[i] = image_label_temp[i]
            y2[i] = self.facs_labels[ids]  # Assign FACS labels

        return X, [keras.utils.to_categorical(y, num_classes=self.n_classes), y2]


    def plot_batch_images(self, batch_index=0):
        'Plots images from a specific batch'
        if batch_index >= len(self):
          raise ValueError("Batch index out of range")

        X_batch, _ = self.__getitem__(batch_index)
        num_images = min(self.batch_size, 16)  # Maximum of 16 subplots

        num_rows = int(np.sqrt(num_images))
        num_cols = (num_images + num_rows - 1) // num_rows

        plt.figure(figsize=(12, 12))
        for i in range(num_images):
          plt.subplot(num_rows, num_cols, i + 1)
          plt.imshow(X_batch[i, :, :, 0], cmap='gray')
          plt.axis('off')
        plt.tight_layout()
        plt.show()

    def __read_data_instance(self, pid):

        filepath = self.data_prefix + self.data_frame.iloc[pid]['filepath']
        data = Image.open(filepath).convert("L")
        original_size = data.size

        # Resize the image to the desired input size
        data = data.resize(self.dim[:-1])

        data = np.asarray(data)
        data = np.expand_dims(data, axis=-1)

        if self.Augment:
            # Randomly apply rotation
            rot = np.random.rand(1) < 0.5
            if rot:
                rot = np.random.randint(-10, 10, size=1)
                data = rotate(data, angle=rot[0], reshape=False)

            # Randomly apply shift
            shift_val = np.random.randint(-5, high=5, size=2, dtype=int).tolist() + [0,]
            data = shift(data, shift_val, order=0, mode='constant', cval=0.0, prefilter=False)

            # Calculate required padding for each dimension
            pad_width = [(0, 0)]  # No padding along the channel dimension
            for i in range(2):  # Loop through height and width dimensions
                pad_needed = self.dim[i] - data.shape[i]
                if pad_needed > 0:
                    pad_width.append((pad_needed // 2, pad_needed - pad_needed // 2))
                else:
                    pad_width.append((0, 0))
            pad_width = tuple(pad_width)

            # Apply padding
            data = np.pad(data, pad_width, mode='constant')

            # Adjust image brightness with random factor
            brightness_factor = np.random.uniform(0.7, 1.3)
            data = np.clip(data * brightness_factor, 0, 255)

        # Crop the image to match the desired input size
        data = data[:self.dim[0], :self.dim[1], :]

        X = data
        X = (X - self.data_mean) / self.data_std
        return X

"""###Configurations for data loader

-  I prepared data generators for training, validation, and testing with the CK dataset. It is normalizing the data, sets up batch sizes, and extracts FACS labels.
- The data generators are configured with the appropriate settings for augmentation, data normalization, and FACS labels to be used in training and evaluation.
"""

data_mean = 0.
data_std = 255.0
prefix = ''
BATCH_SIZE = 16

# Assuming facs_labels is a numpy array of shape (num_samples, num_facs)
facs_labels_train = np.array(train_data_df[['AU17', 'AU1', 'AU2', 'AU25', 'AU27', 'AU4', 'AU7', 'AU23', 'AU24', 'AU6', 'AU12', 'AU15', 'AU14', 'AU11', 'AU26']])
facs_labels_val = np.array(val_data_df[['AU17', 'AU1', 'AU2', 'AU25', 'AU27', 'AU4', 'AU7', 'AU23', 'AU24', 'AU6', 'AU12', 'AU15', 'AU14', 'AU11', 'AU26']])

training_generator = DataGenerator(train_data_df, batch_size=BATCH_SIZE, data_mean=data_mean, data_std=data_std, n_classes=3, Augment=True, data_prefix=prefix, facs_labels=facs_labels_train)
validation_generator = DataGenerator(val_data_df, batch_size=BATCH_SIZE, data_mean=data_mean, data_std=data_std, n_classes=3, Augment=False, data_prefix=prefix, facs_labels=facs_labels_val)

test_generator = DataGenerator(test_data_df, batch_size=BATCH_SIZE, data_mean=data_mean, data_std=data_std, n_classes=3, Augment=False, data_prefix=prefix, facs_labels=facs_labels_train)

"""### Image visualization after augmentation

I have plotted images from the first batch of the training generator to see augmentation reluts and dataloder in case if it is not deteriorating the images.
Please note - Here augmentation can be turned off and on as per the training evaluation outcome.
"""

training_generator.plot_batch_images(batch_index=1)

"""##Augmentation Types


Augmentation involves applying various transformations to the images in the training dataset to artificially increase its diversity and prevent overfitting. Here's an explanation of each augmentation technique that I have used and its potential impact on the CK dataset and its associated emotion and FACS label outcomes:

> - Random Rotation: A random value is generated to decide whether rotation should be applied.If selected, a random rotation angle is chosen between -10 and 10 degrees. The rotate() function is used to apply rotation to the image.
Impact: Rotation augmentation can be useful for improving the model's robustness to different head orientations. It's particularly beneficial for scenarios where emotions are expressed at various angles, such as tilted heads.

> - Random Shift: Random shift values are generated within the range of -5 to 5 pixels for both height and width dimensions.The shift() function is used to apply the shift to the image.
Impact: Shifting augmentation will help the model generalize better to slightly shifted facial expressions, which may occur due to different positioning of the face in the image.

> - Padding: Padding is calculated based on the difference between the desired input size and the image's current size. Padding is added to the image along height and width dimensions, ensuring it matches the desired input dimensions.
Impact: Padding ensures that images of different sizes are effectively adjusted to the same size required by the model. This addresses variations in image dimensions across the dataset.

> - Brightness Adjustment: A random brightness factor is applied to adjust the image's overall brightness.
The clip() function is used to ensure pixel values stay within the 0-255 range.
Impact: Brightness adjustment helps the model become invariant to variations in lighting conditions. This can be crucial for real-world scenarios where lighting can affect facial expressions.
Cropping:

> - Finally, the image is cropped to match the desired input size after all other augmentations. Impact: Cropping ensures that the final image size aligns with the model's input size. It eliminates any excess padding that might have been added.

> Reasons to consider these augmentation on CK Dataset for  Emotion Labels:

- Augmentations like rotation, shifting, and padding can help the model better understand facial expressions presented at different orientations and positions, which is especially important given the variability in facial expressions within the CK dataset.
- Brightness adjustment ensures that the model is robust to lighting conditions, an essential factor in capturing genuine facial expressions.
Overall, these augmentations enhance the model's ability to recognize emotions accurately across various real-world scenarios.


> Reasons to consider these augmentation on CK Dataset for  FACS Labels:

 - FACS labels, representing specific facial muscle activations, can also benefit from these augmentations.
Rotation, shifting, and padding can simulate changes in head angles and positioning, which are likely to affect the activation of different action units.
- Brightness adjustment helps the model handle variations in lighting conditions that may affect the visibility of facial features relevant to FACS labels.

### Output shape after data laoder and batch analysis
"""

print(training_generator.__len__())

for x, (y1, y2) in training_generator.__iter__():
    print("Data shape:", x.shape)
    print("Emotion Label shape:", y1.shape)
    print("FACS Label shape:", y2.shape)
    for z in range(8):
      print('Facs value for batch id', z,'is : ', y2[z])
    print("****************************************************************")

"""###Output Interpretation of shapes post dataloder

- As you can see above, the printed output provides a comprehensive overview of the batch processing.
- It presents the dimensions of the image data, one-hot encoded emotion labels, and FACS labels for each batch.
- The FACS label values for a subset of samples within each batch give an understanding of the specific action units activated or deactivated in those samples.

#Baseline  Model - Restnet

Reasons of choosing Restnet as baseline model :

- Deep Architecture:ResNet's deep structure is well-suited for capturing complex patterns in facial expressions.

- Vanishing Gradient: Residual connections mitigate vanishing gradient issues, enabling effective training of deep networks.

- Feature Reuse:Residual blocks allow reuse of earlier learned features, enhancing model robustness.

- Improved Learning:Residual connections aid in learning identity mappings and underlying relationships.

- Generalization:Batch normalization and skip connections help prevent overfitting in small datasets like CK.

- Transfer Learning:Pretrained ResNet models can be used for initialization, leveraging knowledge from other datasets.

- Efficient Training:Despite depth, ResNet's architecture enables efficient gradient flow and training.

- Hierarchical Features:ResNet learns hierarchical details, crucial for capturing diverse facial expressions.
State-of-the-Art Performance:

ResNet models have excelled in image tasks, implying potential improvement for CK dataset.

###Writing custom CNN layer
The code below is advance code which will make the final code more readable.

- When building networks like ResNet,  we tend to repeat NN block with the same structure over and over again to build a deep model.

- While we can use the sequential/functional API with the existing blocks in tensorflow/keras to build such a large model, the code will become unreadble when the network size increases.

- A solution to this is to create custom layer for a local structure that can then be reapeated.

- Below I am going to create such a local block. This block is the resudial block from ResNet.
> Note : Reference taken from LAB 6.



Here, the ResidualBlock class encapsulates the behavior of a residual block, a critical building block for deep neural networks. By facilitating the propagation of gradients through deep architectures and enabling the construction of networks with multiple layers, residual blocks enhance the model's training efficiency and overall performance.



- ResidualBlock Class: Inherits from the TensorFlow tf.keras.layers.Layer base class, serving as a custom layer.
- __init__ method initializes the residual block's layers and parameters, including convolutional layers, batch normalization, and shortcut connections.
- Layers in the Residual Block:
  - conv1: First convolutional layer with specified filters, kernel size (3x3), and stride.
 - bn1: Batch normalization layer applied after the first convolution.
 - conv2: Second convolutional layer with the same filter count and kernel size.
 - bn2: Batch normalization layer applied after the second convolution.
"""

import tensorflow as tf

class ResidualBlock(tf.keras.layers.Layer):
    def __init__(self, nFilters, stride=1, reg_lambda=0.0):
        super(ResidualBlock, self).__init__()

        # First convolutional layer of the residual block
        self.conv1 = tf.keras.layers.Conv2D(filters=nFilters,
                                            kernel_size=(3, 3),
                                            strides=stride,
                                            kernel_initializer="he_normal",
                                            kernel_regularizer=tf.keras.regularizers.l2(reg_lambda),
                                            padding="same")
        self.bn1 = tf.keras.layers.BatchNormalization(momentum=0.4)

        # Second convolutional layer of the residual block
        self.conv2 = tf.keras.layers.Conv2D(filters=nFilters,
                                            kernel_size=(3, 3),
                                            strides=1,
                                            kernel_initializer="he_normal",
                                            kernel_regularizer=tf.keras.regularizers.l2(reg_lambda),
                                            padding="same")
        self.bn2 = tf.keras.layers.BatchNormalization(momentum=0.4)

        # Shortcut connection (if stride > 1 or number of filters changes)
        if stride > 1:
            self.shortcut = tf.keras.Sequential([
                tf.keras.layers.Conv2D(filters=nFilters,
                                       kernel_size=(1, 1),
                                       strides=stride,
                                       kernel_initializer="he_normal",
                                       kernel_regularizer=tf.keras.regularizers.l2(reg_lambda),
                                       padding="same"),
                tf.keras.layers.BatchNormalization(momentum=0.4)
            ])
        else:
            self.shortcut = lambda x: x

    def call(self, inputs, training=False):
        x = inputs

        x = self.conv1(x)
        x = self.bn1(x)
        x = tf.keras.activations.relu(x)

        x = self.conv2(x)
        x = self.bn2(x)

        shortcut = self.shortcut(inputs)

        x = tf.keras.layers.add([x, shortcut])
        x = tf.keras.activations.relu(x)

        return x

"""###Model Description


> Model Architecture:

- Input layer defined with a specified input shape.
- Initial convolutional layer with 64 filters, kernel size (3x3), and batch normalization.
- Iterates through each stage defined by filters and block_size:
Creates a ResidualBlock with the specified filter count and stride of 2 (for down-sampling) in the first block.
- Creates additional ResidualBlocks with stride 1 in the same stage.
- Global average pooling layer applied to the final stage's output.
Flattening layer converts the pooled output to a 1D vector.

> Output Branches:

- Emotion prediction branch: Dense layer with softmax activation, predicting emotion classes.
- FACS label prediction branch: Dense layer with sigmoid activation, predicting FACS labels.
"""

def get_resnet_model(filters, block_size, reg_lambda=0.0, num_emotion_classes=3, num_facs_labels=15):
    input_layer = tf.keras.layers.Input(shape=(490, 490, 3))  # Adjust the input shape

    x = tf.keras.layers.Conv2D(filters=64,
                               kernel_size=(3, 3),
                               strides=1,
                               kernel_initializer="he_normal",
                               kernel_regularizer=tf.keras.regularizers.l2(reg_lambda),
                               padding="same")(input_layer)
    x = tf.keras.layers.BatchNormalization(momentum=0.4)(x)

    for nFilters, nBlocks in zip(filters, block_size):
        x = ResidualBlock(nFilters, stride=2, reg_lambda=reg_lambda)(x)

        for _ in range(1, nBlocks):
            x = ResidualBlock(nFilters, stride=1, reg_lambda=reg_lambda)(x)

    x = tf.keras.layers.GlobalAveragePooling2D()(x)
    x = tf.keras.layers.Flatten()(x)

    emotion_output = tf.keras.layers.Dense(num_emotion_classes, activation='softmax', name='emotion_output')(x)
    facs_output = tf.keras.layers.Dense(num_facs_labels, activation='sigmoid', name='facs_output')(x)

    model = tf.keras.Model(inputs=input_layer, outputs=[emotion_output, facs_output])
    return model


# Example usage
filters = [64, 128, 256]
block_size = [3, 4, 6]
reg_lambda = 0.001
num_emotion_classes = 3
num_facs_labels = 15

model = get_resnet_model(filters, block_size, reg_lambda, num_emotion_classes=num_emotion_classes, num_facs_labels=num_facs_labels)
model.summary()

"""###Restnet Model Summary   

- My model is a variant of ResNet designed for emotion classification and FACS label prediction.
- It takes RGB images of size 490x490 pixels as input.
- It uses multiple residual blocks to capture image features effectively, with batch normalization for stabilization.
-  The blocks include convolutional layers and residual connections.
- The output goes through global average pooling and flattening. The final layers consist of a dense layer for emotion classification (3 classes) and another for FACS label prediction (15 labels).
- The model has over 8 million parameters, with about 8.1 million trainable. It aims to learn patterns in the data to classify emotions and predict FACS labels accurately.

###Model Param Configuration  

- Model Construction and Compilation:A ResNet model is created using the specified hyperparameters.The model architecture includes multiple residual blocks, each containing convolutional layers, batch normalization, and shortcut connections.The model is compiled with an Adam optimizer using the defined learning rate.Custom loss functions and accuracy metrics are configured for both emotion classification and FACS label prediction.
Training:

- The model is trained using the training data provided by the training_generator. The validation_generator is used for validation during training.Training runs for a specified number of epochs (25 in this case).
Progress and training metrics are displayed for each epoch.
Evaluation:
- The model is evaluated on a test set using the test_generator.Predictions are generated for both emotion classes and FACS labels.

- Emotion classification accuracy is computed using the accuracy_score function.
- FACS label prediction accuracy is determined by comparing rounded predictions with true labels.

"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score
from tensorflow.keras.callbacks import LearningRateScheduler, TensorBoard  # Import the TensorBoard callback

# Define the ResNet model
model = get_resnet_model(filters=[64, 128, 256], block_size=[2, 2, 2], reg_lambda=0.001, num_emotion_classes=3, num_facs_labels=15)

# Define a learning rate schedule function
def lr_schedule(epoch, lr):
    if epoch < 10:
        return lr
    else:
        return lr * np.exp(-0.1)

# Create the LearningRateScheduler callback
lr_scheduler = LearningRateScheduler(lr_schedule)

# Define the log directory for TensorBoard
log_dir = 'content/drive/My Drive/DeepLearningAssignment/ResNetLogDirectory'

# Create the TensorBoard callback
tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)

# Compile the model with TensorBoard and other callbacks
model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.001),
              loss={'emotion_output': 'categorical_crossentropy', 'facs_output': 'binary_crossentropy'},
              loss_weights={'emotion_output': 1.0, 'facs_output': 1.0},
              metrics={'emotion_output': 'accuracy', 'facs_output': 'accuracy'})

# Train the model with both the learning rate scheduler and TensorBoard callbacks
history = model.fit(
    training_generator,
    validation_data=validation_generator,
    epochs=5,
    verbose=1,
    callbacks=[lr_scheduler, tensorboard_callback]  # Include both callbacks
)

# Evaluate on the test set
test_X, test_y = test_generator.__getitem__(0)
emotion_pred, facs_pred = model.predict(test_X)
emotion_true_classes = np.argmax(test_y[0], axis=1)
facs_true = test_y[1]

facs_accuracy = np.mean(np.round(facs_pred) == facs_true)
print("FACS Label Prediction Accuracy:", facs_accuracy)

# Plot the learning rate schedule
learning_rates = [lr_schedule(epoch, lr=0.001) for epoch in range(len(history.history['loss']))]
plt.plot(learning_rates)
plt.xlabel('Epoch')
plt.ylabel('Learning Rate')
plt.title('Learning Rate Schedule')
plt.show()

import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score
from tensorflow.keras.callbacks import LearningRateScheduler

# Define the ResNet model
model = get_resnet_model(filters=[64, 128, 256], block_size=[2, 2, 2], reg_lambda=0.001, num_emotion_classes=3, num_facs_labels=15)

# Define a learning rate schedule function
def lr_schedule(epoch, lr):
    if epoch < 10:
        return lr
    else:
        return lr * np.exp(-0.1)

# Create the LearningRateScheduler callback
lr_scheduler = LearningRateScheduler(lr_schedule)

# Compile the model
model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.001),
              loss={'emotion_output': 'categorical_crossentropy', 'facs_output': 'binary_crossentropy'},
              loss_weights={'emotion_output': 1.0, 'facs_output': 1.0},
              metrics={'emotion_output': 'accuracy', 'facs_output': 'accuracy'})

# Train the model with the lr_scheduler callback
history = model.fit(
    training_generator,
    validation_data=validation_generator,
    epochs=5,
    verbose=1,
    callbacks=[lr_scheduler]
)


# Evaluate on the test set
test_X, test_y = test_generator.__getitem__(0)
emotion_pred, facs_pred = model.predict(test_X)
emotion_true_classes = np.argmax(test_y[0], axis=1)
facs_true = test_y[1]

facs_accuracy = np.mean(np.round(facs_pred) == facs_true)
print("FACS Label Prediction Accuracy:", facs_accuracy)


# Plot the learning rate schedule
learning_rates = [lr_schedule(epoch, lr=0.001) for epoch in range(len(history.history['loss']))]
plt.plot(learning_rates)
plt.xlabel('Epoch')
plt.ylabel('Learning Rate')
plt.title('Learning Rate Schedule')
plt.show()

"""### Restnet Baseline Model - Results Analysis  

Summary of Results:

The model was trained for 5 epochs with a learning rate of 0.001.
Emotion classification accuracy on the validation set reached around 64%, while FACS label prediction accuracy reached approximately 28%.
After evaluating on the test set, the FACS label prediction accuracy improved to around 79%.

> Potential Reasons for High Losses and Accuracy:

1. Limited Training Epochs: Training for only 5 epochs might not be sufficient for the model to fully converge and achieve optimal results. Increasing the number of epochs could potentially improve performance.

2. Learning Rate: The learning rate of 0.001 might not be optimal for this specific dataset and model architecture. Learning rate schedules or optimizers with adaptive learning rates could help stabilize training.

3. Class Imbalance: As there is a significant class imbalance in either the emotion classes, the model is struggling to learn from underrepresented classes, leading to lower accuracy.

4. Data Augmentation: While data augmentation is used, it might not be extensive enough to cover the variability in the dataset, which could impact the model's ability to generalize.
"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score
from tensorflow.keras.callbacks import LearningRateScheduler
from sklearn.metrics import confusion_matrix

# Define the ResNet model
model = get_resnet_model(filters=[64, 128, 256], block_size=[2, 2, 2], reg_lambda=0.001, num_emotion_classes=3, num_facs_labels=15)

# Define a learning rate schedule function
def lr_schedule(epoch, lr):
    if epoch < 10:
        return lr
    else:
        return lr * np.exp(-0.1)

# Create the LearningRateScheduler callback
lr_scheduler = LearningRateScheduler(lr_schedule)

# Define a custom F1 score metric
def custom_f1_score(y_true, y_pred):
    # Ensure that y_pred is a binary tensor (0 or 1)
    y_pred = tf.round(y_pred)

    # Calculate true positives, false positives, and false negatives
    true_positives = tf.reduce_sum(tf.cast(y_true * y_pred, tf.float32))
    false_positives = tf.reduce_sum(tf.cast((1 - y_true) * y_pred, tf.float32))
    false_negatives = tf.reduce_sum(tf.cast(y_true * (1 - y_pred), tf.float32))

    # Calculate precision and recall
    precision = true_positives / (true_positives + false_positives + tf.keras.backend.epsilon())
    recall = true_positives / (true_positives + false_negatives + tf.keras.backend.epsilon())

    # Calculate F1 score (2 * precision * recall / (precision + recall + epsilon))
    f1_score = 2 * (precision * recall) / (precision + recall + tf.keras.backend.epsilon())

    return f1_score

model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.001),
              loss={'emotion_output': 'categorical_crossentropy', 'facs_output': 'binary_crossentropy'},
              loss_weights={'emotion_output': 1.0, 'facs_output': 1.0},
              metrics={'emotion_output': 'accuracy', 'facs_output': custom_f1_score})


# Compile the model
model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.001),
              loss={'emotion_output': 'categorical_crossentropy', 'facs_output': 'binary_crossentropy'},
              loss_weights={'emotion_output': 1.0, 'facs_output': 1.0},
              metrics={'emotion_output': 'accuracy', 'facs_output': 'accuracy'})

# Train the model with the lr_scheduler callback
history = model.fit(
    training_generator,
    validation_data=validation_generator,
    epochs=10,
    verbose=1,
    callbacks=[lr_scheduler]
)



emotion_true_classes = np.argmax(test_y[0], axis=1)

emotion_pred_classes = np.argmax(emotion_pred, axis=1)

emotion_accuracy = np.mean(emotion_pred_classes == emotion_true_classes)

print("Emotion Prediction Accuracy:", emotion_accuracy)
facs_accuracy = np.mean(np.round(facs_pred) == facs_true)
print("FACS Label Prediction Accuracy:", facs_accuracy)

import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import multilabel_confusion_matrix, precision_score, recall_score, f1_score
from tabulate import tabulate

# Assuming you have already trained the model and have test data
test_X, test_y = test_generator.__getitem__(0)
emotion_pred, facs_pred = model.predict(test_X)
facs_true = test_y[1]

# FACS Label Prediction Evaluation
facs_true_rounded = np.round(facs_true)
facs_pred_rounded = np.round(facs_pred)

# Initialize arrays to store precision, recall, and F1 score for each label
facs_precision = []
facs_recall = []
facs_f1 = []

# Calculate precision, recall, and F1 score for each label separately
for label_index in range(facs_true_rounded.shape[1]):
    label_true = facs_true_rounded[:, label_index]
    label_pred = facs_pred_rounded[:, label_index]

    # Compute confusion matrix for the current label
    label_confusion_matrix = multilabel_confusion_matrix(label_true, label_pred)

    # Calculate precision, recall, and F1 score for the current label
    try:
        label_precision = precision_score(label_true, label_pred, zero_division=1)
        label_recall = recall_score(label_true, label_pred, zero_division=1)
        label_f1 = f1_score(label_true, label_pred, zero_division=1)
    except ZeroDivisionError:
        label_precision = 0.0
        label_recall = 0.0
        label_f1 = 0.0

    facs_precision.append(label_precision)
    facs_recall.append(label_recall)
    facs_f1.append(label_f1)

# Create a table with the results
results_table = [["FACS Label", "Precision", "Recall", "F1 Score"]]
for label_index, (precision, recall, f1) in enumerate(zip(facs_precision, facs_recall, facs_f1)):
    results_table.append([f"FACS Label {label_index + 1}", f"{precision:.4f}", f"{recall:.4f}", f"{f1:.4f}"])

# Optionally, you can calculate and print the overall weighted metrics for all labels
weighted_facs_precision = np.average(facs_precision)
weighted_facs_recall = np.average(facs_recall)
weighted_facs_f1 = np.average(facs_f1)

results_table.append(["Weighted Average", f"{weighted_facs_precision:.4f}", f"{weighted_facs_recall:.4f}", f"{weighted_facs_f1:.4f}"])

# Print the table
print(tabulate(results_table, headers="firstrow", tablefmt="fancy_grid"))

print("\nWeighted FACS Label Precision:", weighted_facs_precision)
print("Weighted FACS Label Recall:", weighted_facs_recall)
print("Weighted FACS Label F1 Score:", weighted_facs_f1)

# Plot the results
labels = [f"FACS Label {i+1}" for i in range(len(facs_precision))]
values = facs_precision

plt.bar(labels, values)
plt.ylabel('Precision')
plt.title('FACS Label Precision')
plt.xticks(rotation=45, ha="right")
plt.show()

plt.bar(labels, facs_recall)
plt.ylabel('Recall')
plt.title('FACS Label Recall')
plt.xticks(rotation=45, ha="right")
plt.show()

plt.bar(labels, facs_f1)
plt.ylabel('F1 Score')
plt.title('FACS Label F1 Score')
plt.xticks(rotation=45, ha="right")
plt.show()

import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import precision_score, recall_score, f1_score
from tabulate import tabulate

# Assuming you have already trained the model and have test data
test_X, test_y = test_generator.__getitem__(0)
emotion_pred, facs_pred = model.predict(test_X)
emotion_true_classes = np.argmax(test_y[0], axis=1)

# Emotion Prediction Evaluation
emotion_true_classes = np.argmax(test_y[0], axis=1)
emotion_pred_classes = np.argmax(emotion_pred, axis=1)

# Calculate precision, recall, and F1 score for emotion prediction
emotion_precision = precision_score(emotion_true_classes, emotion_pred_classes, average='weighted', zero_division=1)
emotion_recall = recall_score(emotion_true_classes, emotion_pred_classes, average='weighted', zero_division=1)
emotion_f1 = f1_score(emotion_true_classes, emotion_pred_classes, average='weighted', zero_division=1)

# Print the results for emotion prediction in tabular form
results_table = [["Metric", "Score"],
                ["Precision", f"{emotion_precision:.4f}"],
                ["Recall", f"{emotion_recall:.4f}"],
                ["F1 Score", f"{emotion_f1:.4f}"]]

print(tabulate(results_table, headers="firstrow", tablefmt="fancy_grid"))

# Plot the results
metrics = ['Precision', 'Recall', 'F1 Score']
values = [emotion_precision, emotion_recall, emotion_f1]

plt.bar(metrics, values)
plt.ylabel('Score')
plt.title('Emotion Prediction Metrics')
plt.show()

import numpy as np
from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd

# Calculate emotion prediction accuracy
emotion_true_classes = np.argmax(test_y[0], axis=1)
emotion_pred_classes = np.argmax(emotion_pred, axis=1)
emotion_accuracy = np.mean(emotion_pred_classes == emotion_true_classes)
print("Emotion Prediction Accuracy:", emotion_accuracy)

# Create confusion matrix for emotion predictions
emotion_confusion = confusion_matrix(emotion_true_classes, emotion_pred_classes)

# Create a DataFrame for the emotion confusion matrix
emotion_df = pd.DataFrame(emotion_confusion, columns=["Predicted Emotion " + str(i) for i in range(emotion_confusion.shape[1])],
                         index=["True Emotion " + str(i) for i in range(emotion_confusion.shape[0])])

# Create a heatmap for the emotion confusion matrix
plt.figure(figsize=(6, 6))
sns.heatmap(emotion_df, annot=True, fmt='.2f', cmap='Blues', cbar=False)
plt.title("Emotion Confusion Matrix")
plt.show()

# Calculate FACS label prediction accuracy
facs_accuracy = np.mean(np.round(facs_pred) == facs_true)
print("FACS Label Prediction Accuracy:", facs_accuracy)

# Initialize an empty dictionary to store confusion matrices for each FACS label
facs_multilabel_confusion = {}

# Iterate through each FACS label
num_labels = facs_true.shape[1]  # Replace with the actual number of FACS labels
for label_idx in range(num_labels):
    label_true = facs_true[:, label_idx]
    label_pred = np.round(facs_pred)[:, label_idx]
    label_confusion = confusion_matrix(label_true, label_pred)
    facs_multilabel_confusion[label_idx] = label_confusion

# Create DataFrames for the FACS confusion matrices
facs_dfs = []
for label_idx, label_confusion in facs_multilabel_confusion.items():
    facs_df = pd.DataFrame(label_confusion,
                           columns=["Predicted L " + str(i) for i in range(label_confusion.shape[1])],
                           index=["True L " + str(i) for i in range(label_confusion.shape[0])])
    facs_dfs.append(facs_df)

# Plot FACS confusion matrices separately
for label_idx, facs_df in enumerate(facs_dfs):
    plt.figure(figsize=(6, 6))
    sns.heatmap(facs_df, annot=True, fmt='.2f', cmap='Blues', cbar=False)
    plt.title("FACS Confusion Matrix for Label " + str(label_idx))
    plt.show()

import matplotlib.pyplot as plt

# Plot training history for emotion accuracy and loss
plt.figure(figsize=(12, 6))

# Plot emotion accuracy
plt.subplot(1, 2, 1)
plt.plot(history.history['emotion_output_accuracy'], label='Emotion Accuracy (Training)')
plt.plot(history.history['val_emotion_output_accuracy'], label='Emotion Accuracy (Validation)')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.title('Emotion Accuracy')
plt.legend()

# Plot emotion loss
plt.subplot(1, 2, 2)
plt.plot(history.history['emotion_output_loss'], label='Emotion Loss (Training)')
plt.plot(history.history['val_emotion_output_loss'], label='Emotion Loss (Validation)')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Emotion Loss')
plt.legend()

plt.tight_layout()

# Show the plots
plt.show()

# Plot training history for FACS accuracy and loss
plt.figure(figsize=(12, 6))

# Plot FACS accuracy
plt.subplot(1, 2, 1)
plt.plot(history.history['facs_output_accuracy'], label='FACS Accuracy (Training)')
plt.plot(history.history['val_facs_output_accuracy'], label='FACS Accuracy (Validation)')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.title('FACS Accuracy')
plt.legend()

# Plot FACS loss
plt.subplot(1, 2, 2)
plt.plot(history.history['facs_output_loss'], label='FACS Loss (Training)')
plt.plot(history.history['val_facs_output_loss'], label='FACS Loss (Validation)')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('FACS Loss')
plt.legend()

plt.tight_layout()

# Show the plots
plt.show()

"""### Loss & Accuracy Analysis :

- Training Loss and Validation Loss: The training loss after 5 epochs is around 4.47, while the validation loss is around 4.22. The fact that both losses are relatively high implies that the model is not fitting the data well.
As we know, if the training loss is significantly higher than the validation loss, it suggests that the model is not capturing the complexities of the data and is struggling to fit the training data. In this case,

- Training Accuracy and Validation Accuracy: Similarly, the training accuracy for both emotion classification and FACS label prediction is not very high, around 53% and 31%, respectively. The validation accuracy is also not high, with emotion accuracy around 64% and FACS accuracy around 28%. These relatively low accuracies suggest that the model is not capturing the underlying patterns in the data effectively.

FACS Label Prediction Accuracy on Test Set: The FACS label prediction accuracy on the test set is higher, around 78.75%, which might indicate that the model is generalizing better to unseen data

###Model Configuration for Hyper parameter tuning

- Hyperparameters Setup:The model's hyperparameters are defined, including filter sizes, block sizes, regularization lambda, number of emotion classes, number of FACS labels, and learning rate.
"""

def get_resnet_model(filters, block_size, reg_lambda=0.0, num_emotion_classes=3, num_facs_labels=15):
    input_layer = tf.keras.layers.Input(shape=(490, 490, 3))

    x = tf.keras.layers.Conv2D(filters=64,
                               kernel_size=(3, 3),
                               strides=1,
                               kernel_initializer="he_normal",
                               kernel_regularizer=tf.keras.regularizers.l2(reg_lambda),
                               padding="same")(input_layer)
    x = tf.keras.layers.BatchNormalization(momentum=0.4)(x)

    for nFilters, nBlocks in zip(filters, block_size):
        x = ResidualBlock(nFilters, stride=2, reg_lambda=reg_lambda)(x)

        for _ in range(1, nBlocks):
            x = ResidualBlock(nFilters, stride=1, reg_lambda=reg_lambda)(x)

    x = tf.keras.layers.GlobalAveragePooling2D()(x)
    x = tf.keras.layers.Flatten()(x)

    emotion_output = tf.keras.layers.Dense(num_emotion_classes, activation='softmax', name='emotion_output')(x)
    facs_output = tf.keras.layers.Dense(num_facs_labels, activation='sigmoid', name='facs_output')(x)

    model = tf.keras.Model(inputs=input_layer, outputs=[emotion_output, facs_output])
    return model

def lr_schedule(epoch, lr):
    if epoch < 10:
        return lr
    else:
        return lr * np.exp(-0.1)

# Hyperparameters
filters = [64, 128, 256]
block_size = [3, 4, 6]
reg_lambda = 0.00001
num_emotion_classes = 3
num_facs_labels = 15
learning_rate = 0.001

# Define the log directory for TensorBoard
log_dir = 'content/drive/My Drive/DeepLearningAssignment/ResNetLogDirectory'

# Create the TensorBoard callback
tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)

# Build the ResNet model with updated hyperparameters and learning rate
model = get_resnet_model(filters, block_size, reg_lambda, num_emotion_classes=num_emotion_classes, num_facs_labels=num_facs_labels)
optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)
model.compile(optimizer=optimizer,
              loss={'emotion_output': 'categorical_crossentropy', 'facs_output': 'binary_crossentropy'},
              loss_weights={'emotion_output': 1.0, 'facs_output': 1.0},
              metrics={'emotion_output': 'accuracy', 'facs_output': 'accuracy'})

# Train the model with the lr_scheduler callback
lr_scheduler = tf.keras.callbacks.LearningRateScheduler(lr_schedule)

history = model.fit(
    training_generator,
    validation_data=validation_generator,
    epochs=25,
    verbose=1,
    callbacks=[lr_scheduler , tensorboard_callback]
)


# Evaluate on the test set
test_X, test_y = test_generator.__getitem__(0)
emotion_pred, facs_pred = model.predict(test_X)
emotion_true_classes = np.argmax(test_y[0], axis=1)
facs_true = test_y[1]

emotion_accuracy = accuracy_score(emotion_true_classes, np.argmax(emotion_pred, axis=1))
facs_accuracy = np.mean(np.round(facs_pred) == facs_true)

print("Emotion Classification Accuracy:", emotion_accuracy)
print("FACS Label Prediction Accuracy:", facs_accuracy)

# Plot the learning rate schedule
learning_rates = [lr_schedule(epoch, learning_rate) for epoch in range(25)]
plt.plot(learning_rates)
plt.xlabel('Epoch')
plt.ylabel('Learning Rate')
plt.title('Learning Rate Schedule')
plt.show()

import matplotlib.pyplot as plt

# Plot training history for emotion accuracy and loss
plt.figure(figsize=(12, 6))

# Plot emotion accuracy
plt.subplot(1, 2, 1)
plt.plot(history.history['emotion_output_accuracy'], label='Emotion Accuracy (Training)')
plt.plot(history.history['val_emotion_output_accuracy'], label='Emotion Accuracy (Validation)')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.title('Emotion Accuracy')
plt.legend()

# Plot emotion loss
plt.subplot(1, 2, 2)
plt.plot(history.history['emotion_output_loss'], label='Emotion Loss (Training)')
plt.plot(history.history['val_emotion_output_loss'], label='Emotion Loss (Validation)')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Emotion Loss')
plt.legend()

plt.tight_layout()

# Show the plots
plt.show()

# Plot training history for FACS accuracy and loss
plt.figure(figsize=(12, 6))

# Plot FACS accuracy
plt.subplot(1, 2, 1)
plt.plot(history.history['facs_output_accuracy'], label='FACS Accuracy (Training)')
plt.plot(history.history['val_facs_output_accuracy'], label='FACS Accuracy (Validation)')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.title('FACS Accuracy')
plt.legend()

# Plot FACS loss
plt.subplot(1, 2, 2)
plt.plot(history.history['facs_output_loss'], label='FACS Loss (Training)')
plt.plot(history.history['val_facs_output_loss'], label='FACS Loss (Validation)')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('FACS Loss')
plt.legend()

plt.tight_layout()

# Show the plots
plt.show()

# Updated Hyperparameters
filters = [64, 128, 256]
block_size = [3, 4, 6]
reg_lambda = 0.00001  # Try a smaller regularization value
num_emotion_classes = 3
num_facs_labels = 15
learning_rate = 0.0001  # Try a smaller learning rate

def lr_schedule(epoch, lr):
    if epoch < 10:
        return lr
    else:
        return lr * np.exp(-0.1)

# Build the ResNet model with updated hyperparameters and learning rate
model = get_resnet_model(filters, block_size, reg_lambda, num_emotion_classes=num_emotion_classes, num_facs_labels=num_facs_labels)
optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)
model.compile(optimizer=optimizer,
              loss={'emotion_output': 'categorical_crossentropy', 'facs_output': 'binary_crossentropy'},
              loss_weights={'emotion_output': 1.0, 'facs_output': 1.0},
              metrics={'emotion_output': 'accuracy', 'facs_output': 'accuracy'})

# Train the model with the lr_scheduler callback
lr_scheduler = tf.keras.callbacks.LearningRateScheduler(lr_schedule)

# Define the log directory for TensorBoard
log_dir = 'content/drive/My Drive/DeepLearningAssignment/ResNetLogDirectory'

# Create the TensorBoard callback
tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)


history = model.fit(
    training_generator,
    validation_data=validation_generator,
    epochs=5,
    verbose=1,
    callbacks=[lr_scheduler, tensorboard_callback]
)


# Evaluate on the test set
test_X, test_y = test_generator.__getitem__(0)
emotion_pred, facs_pred = model.predict(test_X)
emotion_true_classes = np.argmax(test_y[0], axis=1)
facs_true = test_y[1]

emotion_accuracy = accuracy_score(emotion_true_classes, np.argmax(emotion_pred, axis=1))
facs_accuracy = np.mean(np.round(facs_pred) == facs_true)

print("Emotion Classification Accuracy:", emotion_accuracy)
print("FACS Label Prediction Accuracy:", facs_accuracy)

# Plot the learning rate schedule
learning_rates = [lr_schedule(epoch, learning_rate) for epoch in range(25)]
plt.plot(learning_rates)
plt.xlabel('Epoch')
plt.ylabel('Learning Rate')
plt.title('Learning Rate Schedule')
plt.show()

import matplotlib.pyplot as plt

# Plot training history for emotion accuracy and loss
plt.figure(figsize=(12, 6))

# Plot emotion accuracy
plt.subplot(1, 2, 1)
plt.plot(history.history['emotion_output_accuracy'], label='Emotion Accuracy (Training)')
plt.plot(history.history['val_emotion_output_accuracy'], label='Emotion Accuracy (Validation)')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.title('Emotion Accuracy')
plt.legend()

# Plot emotion loss
plt.subplot(1, 2, 2)
plt.plot(history.history['emotion_output_loss'], label='Emotion Loss (Training)')
plt.plot(history.history['val_emotion_output_loss'], label='Emotion Loss (Validation)')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Emotion Loss')
plt.legend()

plt.tight_layout()

# Show the plots
plt.show()

# Plot training history for FACS accuracy and loss
plt.figure(figsize=(12, 6))

# Plot FACS accuracy
plt.subplot(1, 2, 1)
plt.plot(history.history['facs_output_accuracy'], label='FACS Accuracy (Training)')
plt.plot(history.history['val_facs_output_accuracy'], label='FACS Accuracy (Validation)')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.title('FACS Accuracy')
plt.legend()

# Plot FACS loss
plt.subplot(1, 2, 2)
plt.plot(history.history['facs_output_loss'], label='FACS Loss (Training)')
plt.plot(history.history['val_facs_output_loss'], label='FACS Loss (Validation)')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('FACS Loss')
plt.legend()

plt.tight_layout()

# Show the plots
plt.show()

# Updated Hyperparameters
filters = [64, 128, 256]
block_size = [3, 4, 6]
reg_lambda = 0.000001  # Try a smaller regularization value
num_emotion_classes = 3
num_facs_labels = 15
learning_rate = 0.1  # Try a smaller learning rate

# Build the ResNet model with updated hyperparameters and learning rate
model = get_resnet_model(filters, block_size, reg_lambda, num_emotion_classes=num_emotion_classes, num_facs_labels=num_facs_labels)
optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)
model.compile(optimizer=optimizer,
              loss={'emotion_output': 'categorical_crossentropy', 'facs_output': 'binary_crossentropy'},
              loss_weights={'emotion_output': 1.0, 'facs_output': 1.0},
              metrics={'emotion_output': 'accuracy', 'facs_output': 'accuracy'})

# Define the log directory for TensorBoard
log_dir = 'content/drive/My Drive/DeepLearningAssignment/ResNetLogDirectory'

# Create the TensorBoard callback
tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)

# Train the model
history = model.fit(
    training_generator,
    validation_data=validation_generator,
    epochs=50,
    verbose=1
)

# Evaluate on the test set
test_X, test_y = test_generator.__getitem__(0)
emotion_pred, facs_pred = model.predict(test_X)
emotion_true_classes = np.argmax(test_y[0], axis=1)
facs_true = test_y[1]

emotion_accuracy = accuracy_score(emotion_true_classes, np.argmax(emotion_pred, axis=1))
facs_accuracy = np.mean(np.round(facs_pred) == facs_true)

print("Emotion Classification Accuracy:", emotion_accuracy)
print("FACS Label Prediction Accuracy:", facs_accuracy)

import matplotlib.pyplot as plt

# Plot training history for emotion accuracy and loss
plt.figure(figsize=(12, 6))

# Plot emotion accuracy
plt.subplot(1, 2, 1)
plt.plot(history.history['emotion_output_accuracy'], label='Emotion Accuracy (Training)')
plt.plot(history.history['val_emotion_output_accuracy'], label='Emotion Accuracy (Validation)')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.title('Emotion Accuracy')
plt.legend()

# Plot emotion loss
plt.subplot(1, 2, 2)
plt.plot(history.history['emotion_output_loss'], label='Emotion Loss (Training)')
plt.plot(history.history['val_emotion_output_loss'], label='Emotion Loss (Validation)')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Emotion Loss')
plt.legend()

plt.tight_layout()

# Show the plots
plt.show()

# Plot training history for FACS accuracy and loss
plt.figure(figsize=(12, 6))

# Plot FACS accuracy
plt.subplot(1, 2, 1)
plt.plot(history.history['facs_output_accuracy'], label='FACS Accuracy (Training)')
plt.plot(history.history['val_facs_output_accuracy'], label='FACS Accuracy (Validation)')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.title('FACS Accuracy')
plt.legend()

# Plot FACS loss
plt.subplot(1, 2, 2)
plt.plot(history.history['facs_output_loss'], label='FACS Loss (Training)')
plt.plot(history.history['val_facs_output_loss'], label='FACS Loss (Validation)')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('FACS Loss')
plt.legend()

plt.tight_layout()

# Show the plots
plt.show()

import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import precision_score, recall_score, f1_score
from tabulate import tabulate

# Assuming you have already trained the model and have test data
test_X, test_y = test_generator.__getitem__(0)
emotion_pred, facs_pred = model.predict(test_X)
emotion_true_classes = np.argmax(test_y[0], axis=1)

# Emotion Prediction Evaluation
emotion_true_classes = np.argmax(test_y[0], axis=1)
emotion_pred_classes = np.argmax(emotion_pred, axis=1)

# Calculate precision, recall, and F1 score for emotion prediction
emotion_precision = precision_score(emotion_true_classes, emotion_pred_classes, average='weighted', zero_division=1)
emotion_recall = recall_score(emotion_true_classes, emotion_pred_classes, average='weighted', zero_division=1)
emotion_f1 = f1_score(emotion_true_classes, emotion_pred_classes, average='weighted', zero_division=1)

# Print the results for emotion prediction in tabular form
results_table = [["Metric", "Score"],
                ["Precision", f"{emotion_precision:.4f}"],
                ["Recall", f"{emotion_recall:.4f}"],
                ["F1 Score", f"{emotion_f1:.4f}"]]

print(tabulate(results_table, headers="firstrow", tablefmt="fancy_grid"))

# Plot the results
metrics = ['Precision', 'Recall', 'F1 Score']
values = [emotion_precision, emotion_recall, emotion_f1]

plt.bar(metrics, values)
plt.ylabel('Score')
plt.title('Emotion Prediction Metrics')
plt.show()

print("\nWeighted FACS Label Precision:", weighted_facs_precision)
print("Weighted FACS Label Recall:", weighted_facs_recall)
print("Weighted FACS Label F1 Score:", weighted_facs_f1)

# Plot the results
labels = [f"FACS Label {i+1}" for i in range(len(facs_precision))]
values = facs_precision

plt.bar(labels, values)
plt.ylabel('Precision')
plt.title('FACS Label Precision')
plt.xticks(rotation=45, ha="right")
plt.show()

plt.bar(labels, facs_recall)
plt.ylabel('Recall')
plt.title('FACS Label Recall')
plt.xticks(rotation=45, ha="right")
plt.show()

plt.bar(labels, facs_f1)
plt.ylabel('F1 Score')
plt.title('FACS Label F1 Score')
plt.xticks(rotation=45, ha="right")
plt.show()

"""**Result Analysis & Ultimate Judgement**

**Overfitting (Run 3)**: Run 3 exhibits signs of overfitting, where the model performs exceptionally well on the training data but struggles to generalize to the validation set. The high emotion accuracy (0.875) on the training data is significantly better than the validation accuracy (0.7946). This suggests that the model has learned to fit the training data too closely, capturing noise or specific patterns that do not generalize well.

**Underfitting (Run 2)**: Run 2 seems to suffer from underfitting. The low emotion accuracy (0.4375) on the training data indicates that the model did not learn the underlying patterns in the data effectively. This may be due to the relatively high regularization lambda (0.01) and insufficient training epochs (15). The model's performance on both training and validation data is suboptimal.

**Balanced (Run 1 and Run 4):** Runs 1 and 4 demonstrate more balanced performance. While they achieve a reasonable emotion accuracy on the training data (0.75), their generalization to the validation data is consistent (0.8839 and 0.8839, respectively). So, model has learned patterns without overfitting or underfitting.

**Ultimate judgment**: Run 1 and Run 4 strike a balance between overfitting and underfitting, showcasing the importance of careful hyperparameter selection for achieving good model performance. Run 3 overfits the data, while Run 2 underfits.

**Accuracy & F 1 score Metrics For emotion prediction:** The precision score of 0.7656 indicates that when the model predicts an emotion, it is correct approximately 76.56% of the time. The recall score of 0.625 suggests that the model captures approximately 62.5% of all actual positive emotions.The F1 score of 0.4808 is the harmonic mean of precision and recall, providing a balance between the two.

**Accuracy & F 1 score Metrics For FACS label detection:** The weighted FACS label precision score of 0.9778 indicates that when the model predicts FACS labels, it is correct approximately 97.78% of the time. Weighted precision considers class imbalance. The weighted FACS label recall score of 0.3111 suggests that the model captures approximately 31.11% of all actual positive FACS labels, again considering class imbalance. The weighted FACS label F1 score of 0.3111 is the harmonic mean of precision and recall for FACS label detection.

In conclusion, the emotion prediction model demonstrates reasonably good performance, with precision and recall scores indicating a balance between precision and recall. However, there is room for improvement in predicting emotions, as the F1 score could be higher. On the other hand, the FACS label detection model shows high precision but relatively low recall, which suggests that it tends to make accurate predictions for FACS labels but may miss some of them. Further optimization may be required to improve recall without sacrificing precision in FACS label detection. Overall, the model's performance can be considered promising but with potential for further refinement and optimization, especially in the context of both emotion prediction and FACS label detection.
"""